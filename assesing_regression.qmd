---
format: 
  revealjs:
    self-contained: false
    slide-number: c/t
    width: 1600
    height: 900
    logo: "https://www.rstudio.com/wp-content/uploads/2018/10/RStudio-Logo-Flat.png"
    footer: "[zahid asghar](https://zahidasghar.com/)"
    theme: ["simple", "custom2.scss"]
    echo: true
    multiplex: true
    code-link: true
    chalkboard: true
    title-slide-attributes:
      data-background-color: "#447099"
editor: source
---

```{r include=FALSE}
library(tidyverse)
library(httr)
clrs <- MetBrewer::met.brewer(name = "Java")
clrs_lt <- colorspace::lighten(clrs, 0.9)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(digits = 3, width = 75)
```

# Internal and external validity of <br> regression studies {background-color="`r clrs[3]`"}

## Goal of this study

-   Internal versus External Validity
-   Testing for Heteroskedasticity

> A statistical analysis has internal validity if the statistical inference made about causal effects are valid for the considered population.

> An analysis is said to have external validity if inferences and conclusion are valid for the studies' population and can be generalized to other populations and settings.

## Internal and External Validity of Regression

-   Internal validity is satisfied if the statistical inference about the causal effects are valid for the population being studied.
-   External validity would need the inferences and conclusions to be generalized from the population and setting studied to other populations and setting.
-   Example: Think about the california test example.
-   Is the slope, βstr , unbiased and consistent?
-   Is this a valid estimate for NY?, WV?, NM?,. . .

## Threats to internal validity

1.  Omitted Variable Bias
2.  Misspecification of Functional Form of Regression Model
3.  Measurement Error
4.  Missing Data and Sample Selection
5.  Simultaneous Causality In each case, OLS assumption $#1$ is violated: $E(u_i| X_{1i}, X_{2i}, \dots,X_{ki})\neq 0.$

## Omitted variable bias

-   If you have the data for omitted variables:
    -   Be specific about your coefficient of interest.
    -   Use a priori reason for adding variables.
    -   Use statistical tests for questionable variables (t and F).
    -   Provide all the potential specifications in tabular form.
-   What if you don't have the data:
    -   Panel data (next chapter)
    -   IV method (chapter 12)
    -   Randomized control experiments

## Misspecification of Functional Form of Regression Model

-   For continuous dependent variable: Use methods discussed in chapter 8 to modify functional forms.
-   For discrete or binary dependent variable: Chapter 11 (we will get there. . . ).

## 

Lets our model is $Y_i = X_i^2$ but one uses $Y_i=\beta_0+\beta_1X_{1i}+u_i$

::: panel-tabset
## Code

```{r}

# set seed for reproducibility
set.seed(5)
library(tidyverse)
# simulate data set
X <- runif(100, -5, 5)
Y <- X^2 + rnorm(100)
df<-cbind(X,Y)
df<-as.data.frame(df)
```

## Output

```{r}
#| eval: false
# estimate the regression function
ms_mod <- lm(Y ~ X)
ms_mod

```
:::

```{r}
# set seed for reproducibility
set.seed(5)
library(tidyverse)
# simulate data set
X <- runif(100, -5, 5)
Y <- X^2 + rnorm(100)
df<-cbind(X,Y)
df<-as.data.frame(df)
# estimate the regression function
ms_mod <- lm(Y ~ X)
ms_mod

```

## Plot this linear fit

```{r}
ggplot(df)+aes(X,Y)+geom_point()+geom_smooth(method = "lm",se=FALSE)

```

## Measurement Error in $X$

General regression model: $Y_i = \beta_0 + \beta_1X_{i} + u_{i}$ What you would like to measure is $X_i$, but what you do actually measure is $\overset{\sim}{X}_i$. Then the error is $X_i - \overset{\sim}{X}_i$

-   Regression model with mesurement error $\overset{\sim}{X}_i$ instead of $X_i$ $\begin{align*} Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + \underbrace{\beta_1 (X_i - \overset{\sim}{X}_i) + u_i}_{=v_i} \\ Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + v_i \end{align*}$

Rewrite $ν_i = \beta_1 (X_i - \overset{\sim}{X}_i)+u_i$

## 

$$\begin{align*}
  Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + \underbrace{\beta_1 (X_i - \overset{\sim}{X}_i) + u_i}_{=v_i} \\
  Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + v_i
\end{align*}$$

where here $\overset{\sim}{X}_i$ and $v_i$ are correlated resulting to slope coefficient $\hat{\beta_1}$ to be biased.

## 

The classical measurement error model assumes that the measurement error, $w_i$, has zero mean and that it is uncorrelated with the variable, $\overset{\sim}{X}_i$, and the error term of the population regression model, $u_i$: $\begin{equation} \overset{\sim}{X}_i = X_i + w_i, \ \ \rho_{w_i,u_i}=0, \ \ \rho_{w_i,X_i}=0 \end{equation}$ This holds $\begin{equation} \widehat{\beta}_1 \xrightarrow{p}{\frac{\sigma_{X}^2}{\sigma_{X}^2 + \sigma_{w}^2}} \beta_1 \tag{9.1} \end{equation}$

## 

$\sigma_{X}^2, \sigma_{w}^2 > 0$ so $\hat{\beta_1}$ smaller than 1.

1.  If there is no measurement error, $\sigma_{w}^2=0$ such that $\widehat{\beta}_1 \xrightarrow{p}{\beta_1}$ .

2.  If $\sigma_{w}^2 \gg \sigma_{X}^2$ we have $\widehat{\beta}_1 \xrightarrow{p}{0}$

This is the case if the measurement error is so large that there essentially is no information on $X$ in the data that can be used to estimate $\beta$.

## 

$\begin{align} (X, Y) \sim \mathcal{N}\left[\begin{pmatrix}50\\ 100\end{pmatrix},\begin{pmatrix}10 & 5 \\ 5 & 10 \end{pmatrix}\right] \tag{9.3} \end{align}$

$\begin{align*} Y_i =& \, 100 + 0.5 (X_i - 50) \\ =& \, 75 + 0.5 X_i. \tag{9.4} \end{align*}$

$\overset{\sim}{X_i} = X_i + w_i$ $w_i \overset{i.i.d.}{\sim} \mathcal{N}(0,10)$ $w_i$ is independent of $x_i$.

## 

```{r}
# set seed
set.seed(1)

# load the package 'mvtnorm' and simulate bivariate normal data
library(mvtnorm)
dat <- data.frame(
  rmvnorm(1000, c(50, 100), 
          sigma = cbind(c(10, 5), c(5, 10))))

# set columns names
colnames(dat) <- c("X", "Y")

```

## 

```{r}
# estimate the model (without measurement error)
noerror_mod <- lm(Y ~ X, data = dat)

# estimate the model (with measurement error in X)
dat$X <- dat$X + rnorm(n = 1000, sd = sqrt(10))
error_mod <- lm(Y ~ X, data = dat)

# print estimated coefficients to console
noerror_mod$coefficients
error_mod$coefficients
```

## Plots

```{r}
#| echo: false
# plot sample data
plot(dat$X, dat$Y, 
     pch = 20, 
     col = "steelblue",
     xlab = "X",
     ylab = "Y")

# add population regression function
abline(coef = c(75, 0.5), 
       col = "darkgreen",
       lwd  = 1.5)

# add estimated regression functions
abline(noerror_mod, 
       col = "purple",
       lwd  = 1.5)

abline(error_mod, 
       col = "darkred",
       lwd  = 1.5)
# add legend
legend("topleft",
       bg = "transparent",
       cex = 0.8,
       lty = 1,
       col = c("darkgreen", "purple", "darkred"), 
       legend = c("Population", "No Errors", "Errors"))

```

## plot in ggplot

```{r}


# estimate the model (with measurement error in X)
dat$X1 <- dat$X + rnorm(n = 1000, sd = sqrt(10))
error_mod <- lm(Y ~ X, data = dat)
ggplot(dat)+aes(X,Y)+geom_smooth(aes(X, Y), data = dat, 
              method = "lm", se = FALSE, color = "red") +
  geom_smooth(aes(X1, Y), data = dat, 
              method = "lm", se = FALSE, color = "blue") +
  geom_point()

```

## Missing Data and Sample Selection

1.  Missing at random: There is no bias, but it reduces our sample size.
2.  Missing regressor values: Same as above.
3.  Missing $Y$ due to selection process (sample selection bias): For example of this type of bias, think about why older, say, baseball players are on average better than younger ones.

## 4. Sample Selection Bias

## 5. Simultaneous Causality

Simultaneous Causality Bias So far we have assumed that the changes in the independent variable $X$ are responsible for changes in the dependent variable $Y$ . When the reverse is also true, we say that there is simultaneous causality between $X$ and $Y$ . This reverse causality leads to correlation between $X$ and the error in the population regression of interest such that the coefficient on $X$ is estimated with bias.

# Summary {background-color="`r clrs[2]`"}

## Threats to Internal Validity of a Regression Study

The five primary threats to internal validity of a multiple regression study are:

-   Omitted variables

-   Misspecification of functional form

-   Errors in variables (measurement errors in the regressors)

-   Sample selection

-   Simultaneous causality

All these threats lead to failure of the first least squares assumption $E(u_i\vert X_{1i},\dots ,X_{ki}) \neq 0$

so that the OLS estimator is biased and inconsistent.

Furthermore, if one does not adjust for heteroskedasticity and/or serial correlation, incorrect standard errors may be a threat to internal validity of the study.
